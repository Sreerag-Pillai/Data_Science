{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sreerag-Pillai/Data_Science/blob/main/Trading%20bot%20using%20Reinforcement%20Learning%20(Q%20Learning)%20SMA%20Cross%20Over%20strategy_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e497b1b",
      "metadata": {
        "id": "2e497b1b"
      },
      "source": [
        "# Chat GPT RL Model In Backtrader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3784c5e",
      "metadata": {
        "id": "a3784c5e"
      },
      "outputs": [],
      "source": [
        "pip install backtrader gym matplotlib pandas mplfinance plotly seaborn numpy scipy mplfinance yfinance\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install backtrader --upgrade\n"
      ],
      "metadata": {
        "id": "xCH05bomSI6f"
      },
      "id": "xCH05bomSI6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6492a136",
      "metadata": {
        "id": "6492a136"
      },
      "source": [
        "# Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b38836",
      "metadata": {
        "id": "a8b38836"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_and_preprocess_data(filename):\n",
        "    data = pd.read_csv(filename)\n",
        "\n",
        "    # Combine 'Date' and 'Time' columns into a single datetime column\n",
        "    data['datetime'] = pd.to_datetime(data['Date'].astype(str) + ' ' + data['Time'])\n",
        "    data.set_index('datetime', inplace=True)\n",
        "\n",
        "    # Rename columns to match convention\n",
        "    data = data.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'})\n",
        "\n",
        "    # Sort by DateTime in ascending order\n",
        "    data.sort_index(inplace=True)\n",
        "\n",
        "    # Drop the 'Date' and 'Time' columns\n",
        "    data.drop(['Date', 'Time'], axis=1, inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "def compute_sma(data, window_size, column_name):\n",
        "    return data[column_name].rolling(window=window_size).mean()\n",
        "\n",
        "data = load_and_preprocess_data('/content/Dataset 2021 - 5 Minute Data.csv')\n",
        "\n",
        "# Compute fast and slow SMAs\n",
        "fast_window = 1\n",
        "slow_window = 3\n",
        "data['fast_sma'] = compute_sma(data, fast_window, 'close')\n",
        "data['slow_sma'] = compute_sma(data, slow_window, 'close')\n",
        "\n",
        "data = data.dropna()\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8741271f",
      "metadata": {
        "id": "8741271f"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e8c5a94",
      "metadata": {
        "id": "8e8c5a94"
      },
      "source": [
        "# Integrating Q learn agent to SMA crossover strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the agent"
      ],
      "metadata": {
        "id": "TnmwuSTy6Tnm"
      },
      "id": "TnmwuSTy6Tnm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Define the Q-learning Agent"
      ],
      "metadata": {
        "id": "zsXKeoMh6kK6"
      },
      "id": "zsXKeoMh6kK6"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, gamma, epsilon, actions):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.actions = actions\n",
        "        self.Q = {}  # Q-table\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            return np.random.choice(self.actions)\n",
        "        else:\n",
        "            q_values = [self.get_Q(state, action) for action in self.actions]\n",
        "            return self.actions[np.argmax(q_values)]\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        predict = self.get_Q(state, action)\n",
        "        target = reward + self.gamma * max([self.get_Q(next_state, a) for a in self.actions])\n",
        "        self.update_Q(state, action, predict + self.alpha * (target - predict))\n",
        "\n",
        "    def get_Q(self, state, action):\n",
        "        return self.Q.get((state, action), 0.0)\n",
        "\n",
        "    def update_Q(self, state, action, value):\n",
        "        self.Q[(state, action)] = value\n"
      ],
      "metadata": {
        "id": "lqgFXiMv6TLK"
      },
      "id": "lqgFXiMv6TLK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Define the Training Loop"
      ],
      "metadata": {
        "id": "r0RU2IX66nZi"
      },
      "id": "r0RU2IX66nZi"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent_on_data(data, agent, epochs=1):\n",
        "    for _ in range(epochs):\n",
        "        for i in range(1, len(data) - 1):\n",
        "            # Define current state\n",
        "            price_trend = 'rising' if data['close'][i] > data['close'][i-1] else 'falling' if data['close'][i] < data['close'][i-1] else 'stable'\n",
        "            sma_trend = 'bullish' if data['fast_sma'][i] > data['slow_sma'][i] else 'bearish' if data['fast_sma'][i] < data['slow_sma'][i] else 'neutral'\n",
        "            state = (price_trend, sma_trend)\n",
        "\n",
        "            # Agent decides on an action\n",
        "            action = agent.choose_action(state)\n",
        "\n",
        "            # Calculate reward\n",
        "            reward = 0\n",
        "            if action == 1:  # Buy\n",
        "                reward = data['close'][i+1] - data['close'][i]\n",
        "            elif action == 2:  # Sell\n",
        "                reward = data['close'][i] - data['close'][i+1]\n",
        "\n",
        "            # Determine next_state\n",
        "            next_price_trend = 'rising' if data['close'][i+1] > data['close'][i] else 'falling' if data['close'][i+1] < data['close'][i] else 'stable'\n",
        "            next_sma_trend = 'bullish' if data['fast_sma'][i+1] > data['slow_sma'][i+1] else 'bearish' if data['fast_sma'][i+1] < data['slow_sma'][i+1] else 'neutral'\n",
        "            next_state = (next_price_trend, next_sma_trend)\n",
        "\n",
        "            # Update the Q-values\n",
        "            agent.learn(state, action, reward, next_state)\n"
      ],
      "metadata": {
        "id": "ceMrykZz6nx9"
      },
      "id": "ceMrykZz6nx9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Splitting Data & Training"
      ],
      "metadata": {
        "id": "Vceysthq62KH"
      },
      "id": "Vceysthq62KH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the agent\n",
        "train_size = int(0.8 * len(data))\n",
        "train_data = data[:train_size]\n",
        "test_data = data[train_size:]\n",
        "\n",
        "agent = QLearningAgent(alpha=0.1, gamma=0.9, epsilon=0.1, actions=[0, 1, 2])  # 0:Hold, 1:Buy, 2:Sell\n",
        "train_agent_on_data(train_data, agent, epochs=10)\n",
        "trained_Q = agent.Q"
      ],
      "metadata": {
        "id": "kgXK8Rjt62kN"
      },
      "id": "kgXK8Rjt62kN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Backtesting the Q-learning-based Trading Strategy\n",
        "\n"
      ],
      "metadata": {
        "id": "2Xbk-nmY7plK"
      },
      "id": "2Xbk-nmY7plK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "913707e2",
      "metadata": {
        "id": "913707e2"
      },
      "outputs": [],
      "source": [
        "import backtrader as bt\n",
        "import datetime as dt\n",
        "\n",
        "class QLearningStrategy(bt.Strategy):\n",
        "    params = (\n",
        "        ('stop_loss', 0.20),  # 20% Stop Loss\n",
        "    )\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fast_sma = bt.indicators.SimpleMovingAverage(self.data.close, period=1)\n",
        "        self.slow_sma = bt.indicators.SimpleMovingAverage(self.data.close, period=3)\n",
        "\n",
        "        self.agent = QLearningAgent(alpha=0.1, gamma=0.9, epsilon=0.1, actions=[0, 1, 2])\n",
        "        self.agent.Q = trained_Q\n",
        "\n",
        "        self.order = None\n",
        "        self.start_cash = self.broker.getvalue()\n",
        "\n",
        "    def next(self):\n",
        "        # Ensure there's enough data for the slow SMA\n",
        "        if len(self.data) < slow_window:  # Changed from 20 to slow_window\n",
        "            return\n",
        "\n",
        "        # Only trade during regular market hours: 9 am to 2:45 pm\n",
        "        if self.data.datetime.time() < dt.time(9) or self.data.datetime.time() > dt.time(14, 45):\n",
        "            return\n",
        "\n",
        "        if self.order:  # Check if an order is pending\n",
        "            return\n",
        "\n",
        "        price_trend = 'rising' if self.data.close[0] > self.data.close[-1] else 'falling'\n",
        "        sma_trend = 'bullish' if self.fast_sma[0] > self.slow_sma[0] else 'bearish'\n",
        "        state = (price_trend, sma_trend)\n",
        "\n",
        "        action = self.agent.choose_action(state)\n",
        "\n",
        "        if action == 1 and not self.position:\n",
        "            self.order = self.buy(size=3)\n",
        "\n",
        "        elif action == 2 and self.position:\n",
        "            self.order = self.sell(size=3)\n",
        "\n",
        "        # Check for stop loss condition\n",
        "        if self.broker.getvalue() < (1 - self.params.stop_loss) * self.start_cash:\n",
        "            self.close()\n",
        "            self.order = None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tzKcqtL0RhBB"
      },
      "id": "tzKcqtL0RhBB"
    },
    {
      "cell_type": "code",
      "source": [
        "class BuyAndHoldStrategy(bt.Strategy):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def next(self):\n",
        "        # Buy at the start and hold\n",
        "        if not self.position:\n",
        "            self.buy(size=3)\n"
      ],
      "metadata": {
        "id": "kZpgjxFeRhYp"
      },
      "id": "kZpgjxFeRhYp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create cerebro instance\n",
        "cerebro = bt.Cerebro()\n",
        "\n",
        "# Load test data into a data feed and add to cerebro\n",
        "data_feed = bt.feeds.PandasData(dataname=test_data)\n",
        "cerebro.adddata(data_feed)\n",
        "\n",
        "# Add the Q-learning strategy\n",
        "cerebro.addstrategy(QLearningStrategy)\n",
        "\n",
        "# Add constraints and set initial cash\n",
        "cerebro.broker.set_cash(100000.0)\n",
        "cerebro.broker.setcommission(commission=0.001)\n",
        "\n",
        "print('Starting Portfolio Value (QLearning): %.2f' % cerebro.broker.getvalue())\n",
        "cerebro.run()\n",
        "print('Ending Portfolio Value (QLearning): %.2f' % cerebro.broker.getvalue())\n",
        "\n",
        "# Plot the performance\n",
        "cerebro.plot()\n",
        "\n",
        "# Now for the Buy and Hold Strategy\n",
        "cerebro = bt.Cerebro()\n",
        "cerebro.adddata(data_feed)\n",
        "cerebro.addstrategy(BuyAndHoldStrategy)\n",
        "cerebro.broker.set_cash(100000.0)\n",
        "cerebro.broker.setcommission(commission=0.001)\n",
        "\n",
        "print('\\nStarting Portfolio Value (Buy and Hold): %.2f' % cerebro.broker.getvalue())\n",
        "cerebro.run()\n",
        "print('Ending Portfolio Value (Buy and Hold): %.2f' % cerebro.broker.getvalue())\n",
        "\n",
        "cerebro.plot()\n"
      ],
      "metadata": {
        "id": "fh62jhsNSVol"
      },
      "id": "fh62jhsNSVol",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}